sub_t_data=subset(my_data, discourse == id)
sub_t_data=sub_t_data[!duplicated(sub_t_data[,c('element_norm_time')]),]
for (phone in levels(factor(sub_t_data$phone[which(grepl(1, sub_t_data$phone))]))){
stressed_time=c(stressed_time,min(sub_t_data$element_norm_time[which(sub_t_data$phone==phone)]))
}
if(length(stressed_time)<3){
stressed_time=c()
next
}
stressed_time_list[[i]]=sort(stressed_time)-5
stressed_time=c()
discourse_list=c(discourse_list,id)
f0_list[[i]] = sub_t_data$F0_relative
res_focus[[i]]=sub_t_data$Focus_res
res_int[[i]]=sub_t_data$Int_res
res_str[[i]]=sub_t_data$Str_res
speakers[[i]]=sub_t_data$speaker[1]
time_list[[i]] = sub_t_data$element_norm_time-5
# if(max(time_list[[i]])<0.3){
#   next
# }
dur_f0 = c(dur_f0, max(time_list[[i]]))
len_f0=c(len_f0,length(time_list[[i]]))
focus=c(focus,sub_t_data$Focus[1]) #Levels: Third Second First Wide
intonation=c(intonation,sub_t_data$Intonation[1])
structure=c(structure,sub_t_data$Structure[1]) #Levels: (AB)C A(BC)
i<-i+1
if(i%%1000==0){print (i/100)}
}
n_levels=length(f0_list)
n_levels
subsamp= runif(n_levels) < 0.2 # randomly select 2%
trunc(median(len_f0)/2)
mean_dur_f0=mean(dur_f0)
n_knots_vec <- seq(4,20,4) # explore from 10 knots up to 80
loglam_vec <- seq(-4,2,1) # explore lambda from 10^(-4) to 10^2
gcv_err <- array(dim=c(n_levels,length(loglam_vec),length(n_knots_vec)),dimnames=c('items','lambda','knots'))
i_sample <- sample(1:n_levels,50) # a data subset, to save computation time
# compute GCV error for all (n_knots, lambda) combinations on the i_sample curves, store it in gcv_err (may take some minutes)
for (k in 1:length(n_knots_vec)) {
for (l in 1:length(loglam_vec)) {
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots_vec[k])
Lfdobj <- 3 # 2 + order of derivative expected to be used. We need velocity to apply Xu's principle, thus order = 1
norder <- 5 # 2 + Lfdobj
nbasis <- length(knots) + norder - 2 # a fixed relation about B-splines
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj, 10^(loglam_vec[l]))
for (i in i_sample) {
# apply linear time normalization
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
gcv_err[i,l,k] = smooth.basis(t_norm,f0_list[[i]],fdPar)$gcv
}
}
}
# compute log of the median of gcv_err for each n_knots and lambda combination
gcv_log_err = log( apply(gcv_err,2:3,median, na.rm=T)) # median to protect from outliers
# plot log GCV errors on a grid
png(paste(plots_dir,'GCV_log_err_f0.png',sep=''))
col.l <- colorRampPalette(c('blue', 'white'))(30)
levelplot( gcv_log_err, scales=list(y=list(at=1:length(n_knots_vec),labels=n_knots_vec,cex=1.5), x=list(at=1:length(loglam_vec),labels=sapply(loglam_vec,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5) ),xlab = list(label = expression(lambda), cex=2), ylab = list(label= 'k',cex=2),col.regions=col.l,
colorkey=list(label=list(at=-6:-3,label=sapply(-6:-3,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5)),
#aspect = "iso", shrink = c(0.7, 1),
#colorkey=T
)
dev.off()
i
traceback()
mean_dur_f0=mean(dur_f0)
n_knots_vec <- seq(4,20,4) # explore from 10 knots up to 80
loglam_vec <- seq(-4,2,1) # explore lambda from 10^(-4) to 10^2
gcv_err <- array(dim=c(n_levels,length(loglam_vec),length(n_knots_vec)),dimnames=c('items','lambda','knots'))
i_sample <- sample(1:n_levels,50) # a data subset, to save computation time
# compute GCV error for all (n_knots, lambda) combinations on the i_sample curves, store it in gcv_err (may take some minutes)
for (k in 1:length(n_knots_vec)) {
for (l in 1:length(loglam_vec)) {
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots_vec[k])
Lfdobj <- 3 # 2 + order of derivative expected to be used. We need velocity to apply Xu's principle, thus order = 1
norder <- 5 # 2 + Lfdobj
nbasis <- length(knots) + norder - 2 # a fixed relation about B-splines
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj, 10^(loglam_vec[l]))
for (i in i_sample) {
# apply linear time normalization
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
gcv_err[i,l,k] = smooth.basis(t_norm,f0_list[[i]],fdPar)$gcv
}
}
}
traceback()
mean_dur_f0
dur_f0
time_list
sub_t_data$element_norm_time
time_list = list() # list for justified time
f0_list = list() #for f0_relative
res_focus=list()
res_int=list()
res_str=list()
dur_f0 = c() # max justified time
speakers=c()
discourse=factor(my_data$discourse)
discourse_list=c()
focus=c()
intonation=c()
structure=c()
len_f0 = c() # number of samples
stressed_time=c()
stressed_time_list=list()
i=1
for (id in levels(discourse)){
sub_t_data=subset(my_data, discourse == id)
sub_t_data=sub_t_data[!duplicated(sub_t_data[,c('element_norm_time')]),]
for (phone in levels(factor(sub_t_data$phone[which(grepl(1, sub_t_data$phone))]))){
stressed_time=c(stressed_time,min(sub_t_data$element_norm_time[which(sub_t_data$phone==phone)]))
}
if(length(stressed_time)<3){
stressed_time=c()
next
}
stressed_time_list[[i]]=sort(stressed_time)
stressed_time=c()
discourse_list=c(discourse_list,id)
f0_list[[i]] = sub_t_data$F0_relative
res_focus[[i]]=sub_t_data$Focus_res
res_int[[i]]=sub_t_data$Int_res
res_str[[i]]=sub_t_data$Str_res
speakers[[i]]=sub_t_data$speaker[1]
time_list[[i]] = sub_t_data$element_norm_time
# if(max(time_list[[i]])<0.3){
#   next
# }
dur_f0 = c(dur_f0, max(time_list[[i]]))
len_f0=c(len_f0,length(time_list[[i]]))
focus=c(focus,sub_t_data$Focus[1]) #Levels: Third Second First Wide
intonation=c(intonation,sub_t_data$Intonation[1])
structure=c(structure,sub_t_data$Structure[1]) #Levels: (AB)C A(BC)
i<-i+1
if(i%%1000==0){print (i/100)}
}
n_levels=length(f0_list)
mean_dur_f0=mean(dur_f0)
mean_dur_f0
n_knots_vec <- seq(4,20,4) # explore from 10 knots up to 80
loglam_vec <- seq(-4,2,1) # explore lambda from 10^(-4) to 10^2
gcv_err <- array(dim=c(n_levels,length(loglam_vec),length(n_knots_vec)),dimnames=c('items','lambda','knots'))
i_sample <- sample(1:n_levels,50) # a data subset, to save computation time
# compute GCV error for all (n_knots, lambda) combinations on the i_sample curves, store it in gcv_err (may take some minutes)
for (k in 1:length(n_knots_vec)) {
for (l in 1:length(loglam_vec)) {
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots_vec[k])
Lfdobj <- 3 # 2 + order of derivative expected to be used. We need velocity to apply Xu's principle, thus order = 1
norder <- 5 # 2 + Lfdobj
nbasis <- length(knots) + norder - 2 # a fixed relation about B-splines
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj, 10^(loglam_vec[l]))
for (i in i_sample) {
# apply linear time normalization
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
gcv_err[i,l,k] = smooth.basis(t_norm,f0_list[[i]],fdPar)$gcv
}
}
}
gcv_log_err = log( apply(gcv_err,2:3,median, na.rm=T)) # median to protect from outliers
png(paste(plots_dir,'GCV_log_err_f0.png',sep=''))
col.l <- colorRampPalette(c('blue', 'white'))(30)
levelplot( gcv_log_err, scales=list(y=list(at=1:length(n_knots_vec),labels=n_knots_vec,cex=1.5), x=list(at=1:length(loglam_vec),labels=sapply(loglam_vec,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5) ),xlab = list(label = expression(lambda), cex=2), ylab = list(label= 'k',cex=2),col.regions=col.l,
colorkey=list(label=list(at=-6:-3,label=sapply(-6:-3,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5)),
#aspect = "iso", shrink = c(0.7, 1),
#colorkey=T
)
dev.off()
trunc(median(len_f0)/2)
argmin = which(gcv_log_err==min(gcv_log_err), arr.ind=TRUE) # rows are lambda indices, cols are n_knots indices
10^(loglam_vec[argmin[1]])
n_knots_vec[argmin[2]]
n_knots_vec <- seq(10,70,10) # explore from 10 knots up to 80
loglam_vec <- seq(-4,2,1) # explore lambda from 10^(-4) to 10^2
gcv_err <- array(dim=c(n_levels,length(loglam_vec),length(n_knots_vec)),dimnames=c('items','lambda','knots'))
i_sample <- sample(1:n_levels,50) # a data subset, to save computation time
# compute GCV error for all (n_knots, lambda) combinations on the i_sample curves, store it in gcv_err (may take some minutes)
for (k in 1:length(n_knots_vec)) {
for (l in 1:length(loglam_vec)) {
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots_vec[k])
Lfdobj <- 3 # 2 + order of derivative expected to be used. We need velocity to apply Xu's principle, thus order = 1
norder <- 5 # 2 + Lfdobj
nbasis <- length(knots) + norder - 2 # a fixed relation about B-splines
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj, 10^(loglam_vec[l]))
for (i in i_sample) {
# apply linear time normalization
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
gcv_err[i,l,k] = smooth.basis(t_norm,f0_list[[i]],fdPar)$gcv
}
}
}
# compute log of the median of gcv_err for each n_knots and lambda combination
gcv_log_err = log( apply(gcv_err,2:3,median, na.rm=T)) # median to protect from outliers
# plot log GCV errors on a grid
png(paste(plots_dir,'large_GCV_log_err_f0.png',sep=''))
col.l <- colorRampPalette(c('blue', 'white'))(30)
levelplot( gcv_log_err, scales=list(y=list(at=1:length(n_knots_vec),labels=n_knots_vec,cex=1.5), x=list(at=1:length(loglam_vec),labels=sapply(loglam_vec,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5) ),xlab = list(label = expression(lambda), cex=2), ylab = list(label= 'k',cex=2),col.regions=col.l,
colorkey=list(label=list(at=-6:-3,label=sapply(-6:-3,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5)),
#aspect = "iso", shrink = c(0.7, 1),
#colorkey=T
)
dev.off()
for (loglam in c(-4)) {
for (n_knots in c(8,20,50)) {
lambda = 10^(loglam)
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots)
Lfdobj <- 3
norder <- 5
nbasis <- length(knots) + norder - 2
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj,lambda)
i=155 # select here a random curve
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
y_fd = smooth.basis(t_norm,f0_list[[i]],fdPar)$fd
png(paste(plots_dir,'f0_fit_loglam',loglam,'n_knots',n_knots,'.png',sep=''))
plot(y_fd,xlab='Element_norm_time',ylab='F0_relative',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='red',lwd=3)
points(t_norm,f0_list[[i]],pch=20)
#legend('topleft',legend= c(eval(substitute(expression(lambda == 10^x),list(x = loglam))),eval(substitute(expression(k == x),list(x = n_knots))) ),cex=2  )
dev.off()
}
}
for (loglam in c(-4)) {
for (n_knots in c(8,20,50)) {
lambda = 10^(loglam)
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots)
Lfdobj <- 4
norder <- 6
nbasis <- length(knots) + norder - 2
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj,lambda)
i=155 # select here a random curve
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
y_fd = smooth.basis(t_norm,f0_list[[i]],fdPar)$fd
png(paste(plots_dir,'f0_fit_loglam',loglam,'n_knots',n_knots,'.png',sep=''))
plot(y_fd,xlab='Element_norm_time',ylab='F0_relative',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='red',lwd=3)
points(t_norm,f0_list[[i]],pch=20)
#legend('topleft',legend= c(eval(substitute(expression(lambda == 10^x),list(x = loglam))),eval(substitute(expression(k == x),list(x = n_knots))) ),cex=2  )
dev.off()
}
}
for (loglam in c(-4)) {
for (n_knots in c(8,20,50)) {
lambda = 10^(loglam)
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots)
Lfdobj <- 2
norder <- 4
nbasis <- length(knots) + norder - 2
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj,lambda)
i=155 # select here a random curve
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
y_fd = smooth.basis(t_norm,f0_list[[i]],fdPar)$fd
png(paste(plots_dir,'f0_fit_loglam',loglam,'n_knots',n_knots,'.png',sep=''))
plot(y_fd,xlab='Element_norm_time',ylab='F0_relative',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='red',lwd=3)
points(t_norm,f0_list[[i]],pch=20)
#legend('topleft',legend= c(eval(substitute(expression(lambda == 10^x),list(x = loglam))),eval(substitute(expression(k == x),list(x = n_knots))) ),cex=2  )
dev.off()
}
}
for (loglam in c(-4)) {
for (n_knots in c(8,20,50)) {
lambda = 10^(loglam)
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots)
Lfdobj <- 3
norder <- 5
nbasis <- length(knots) + norder - 2
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj,lambda)
i=155 # select here a random curve
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
y_fd = smooth.basis(t_norm,f0_list[[i]],fdPar)$fd
png(paste(plots_dir,'3_f0_fit_loglam',loglam,'n_knots',n_knots,'.png',sep=''))
plot(y_fd,xlab='Element_norm_time',ylab='F0_relative',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='red',lwd=3)
points(t_norm,f0_list[[i]],pch=20)
#legend('topleft',legend= c(eval(substitute(expression(lambda == 10^x),list(x = loglam))),eval(substitute(expression(k == x),list(x = n_knots))) ),cex=2  )
dev.off()
}
}
mean_dur_f0=mean(dur_f0)
n_knots_vec <- seq(5,30,5) # explore from 10 knots up to 80
loglam_vec <- seq(-4,2,1) # explore lambda from 10^(-4) to 10^2
gcv_err <- array(dim=c(n_levels,length(loglam_vec),length(n_knots_vec)),dimnames=c('items','lambda','knots'))
i_sample <- sample(1:n_levels,50) # a data subset, to save computation time
# compute GCV error for all (n_knots, lambda) combinations on the i_sample curves, store it in gcv_err (may take some minutes)
for (k in 1:length(n_knots_vec)) {
for (l in 1:length(loglam_vec)) {
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots_vec[k])
Lfdobj <- 2 # 2 + order of derivative expected to be used. We need velocity to apply Xu's principle, thus order = 1
norder <- 4 # 2 + Lfdobj
nbasis <- length(knots) + norder - 2 # a fixed relation about B-splines
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj, 10^(loglam_vec[l]))
for (i in i_sample) {
# apply linear time normalization
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
gcv_err[i,l,k] = smooth.basis(t_norm,f0_list[[i]],fdPar)$gcv
}
}
}
# compute log of the median of gcv_err for each n_knots and lambda combination
gcv_log_err = log( apply(gcv_err,2:3,median, na.rm=T)) # median to protect from outliers
png(paste(plots_dir,'4_large_GCV_log_err_f0.png',sep=''))
col.l <- colorRampPalette(c('blue', 'white'))(30)
levelplot( gcv_log_err, scales=list(y=list(at=1:length(n_knots_vec),labels=n_knots_vec,cex=1.5), x=list(at=1:length(loglam_vec),labels=sapply(loglam_vec,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5) ),xlab = list(label = expression(lambda), cex=2), ylab = list(label= 'k',cex=2),col.regions=col.l,
colorkey=list(label=list(at=-6:-3,label=sapply(-6:-3,function(x) eval(substitute(expression(10^y) ,list(y=x)) )),cex=1.5)),
#aspect = "iso", shrink = c(0.7, 1),
#colorkey=T
)
dev.off()
plots_dir = paste(root_dir,'FDAplots/word2/',sep='')
color
name
mean_dur_f0=mean(dur_f0)
# selected values:
lambda = 10^(-4) ; n_knots = 20
# build global f0 fd object
norm_rng <- c(0,mean_dur_f0)
knots <- seq(0,mean_dur_f0,length.out = n_knots)
Lfdobj <- 2
norder <- 4
nbasis <- length(knots) + norder - 2
basis <- create.bspline.basis(norm_rng, nbasis, norder, knots)
fdPar <- fdPar(basis, Lfdobj,lambda)
# convenient aliases
basis_res = basis
fdPar_res = fdPar
# smooth.basis() does not accept different time samples for different curves.
# Thus we create smooth curves one by one on the same basis, store the spline coefficients and compose an fd object at the end.
res_coefs = matrix(nrow = nbasis, ncol = n_levels)
for (i in 1:n_levels) {
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
res_coefs[,i] = c(smooth.basis(t_norm,res_list[[i]],fdPar)$fd$coefs)
}
res_fd = fd(coef=res_coefs, basisobj=basis)
# curves are linearly time normalized, their duration is mean_dur_f0
# plot the curves
png(paste(plots_dir,'f0_lin.png',sep=''))
#png(paste(plots_dir,'f0_lin_nocol.png',sep=''))
plot(c(0,mean_dur_f0),c(-5,4),type='n',xlab='element_norm_time (ms)',ylab='Residualized F0',main = '',las=1,cex.axis=1.5,cex.lab=1.5)
for (i in (1:n_levels)[subsamp]) {
#lines(res_fd[i],col = 'black', lty=1,lwd=1)
lines(res_fd[i],col = color[[target[i]]], lty=lty[[target[i]]], lwd=lwd[[target[i]]])
}
legend('topleft',legend=unlist(name),col=unlist(color),lwd=unlist(lwd),lty=unlist(lty),cex=1.5)
dev.off()
target<-structure
res_list<-res_str #structure
for (i in 1:n_levels) {
t_norm = (time_list[[i]] / dur_f0[i]) * mean_dur_f0
res_coefs[,i] = c(smooth.basis(t_norm,res_list[[i]],fdPar)$fd$coefs)
}
res_fd = fd(coef=res_coefs, basisobj=basis)
# curves are linearly time normalized, their duration is mean_dur_f0
# plot the curves
png(paste(plots_dir,'f0_lin.png',sep=''))
#png(paste(plots_dir,'f0_lin_nocol.png',sep=''))
plot(c(0,mean_dur_f0),c(-5,4),type='n',xlab='element_norm_time (ms)',ylab='Residualized F0',main = '',las=1,cex.axis=1.5,cex.lab=1.5)
for (i in (1:n_levels)[subsamp]) {
#lines(res_fd[i],col = 'black', lty=1,lwd=1)
lines(res_fd[i],col = color[[target[i]]], lty=lty[[target[i]]], lwd=lwd[[target[i]]])
}
legend('topleft',legend=unlist(name),col=unlist(color),lwd=unlist(lwd),lty=unlist(lty),cex=1.5)
dev.off()
land = matrix(nrow = n_levels, ncol = length(stressed_time_list[[i]])+2) # one internal landmark + begin and end
for (i in 1:n_levels) {
land[i,] = c(0,stressed_time_list[[i]]/dur_f0[[i]],1) * mean_dur_f0
}
reg = landmarkreg.nocurve(land, nhknots = n_knots)
resreg_coefs =  matrix(nrow = nbasis, ncol = n_levels)
reg_fdPar = fdPar(basis, Lfdobj,1e-12) # lambda small, since smoothing already occurred
# reg$hfunmat is a matrix whose i-th column contains the time samples h(x) for the i-th curve,
# where x (reg$x) are regularly spaced time samples along the registered time axis and h() is the time warping function
# that returns the original time axis points.
for (i in 1:n_levels) {
h_i = reg$hfunmat[,i]
resreg_coefs[,i] = c(smooth.basis(reg$x, eval.fd(h_i,res_fd[i]),reg_fdPar)$fd$coefs)
}
resreg_fd = fd(coef=resreg_coefs, basisobj=basis)
# Graphical parameters for landmark labels: place a label in the middle of every interval.
landlab = c("1st","2nd","3rd","rest")
at_land = c() # position of the label along the time axis
for (i in 1:(length(reg$land)-1)) {
at_land = c(at_land, mean(reg$land[i:(i+1)]))
}
png(paste(plots_dir,'f0_reg.png',sep=''))
# png(paste(plots_dir,'f0_reg_nocol.png',sep=''))
plot(range(reg$land),c(ybot,yup),type='n',xlab='Element_norm_time',ylab='Residualized F0',main = '',las=1,cex.axis=1.5,cex.lab=1.5)
for (i in (1:n_levels)[subsamp]) {
# lines(resreg_fd[i],col = 'black', lty=1,lwd=1)
lines(resreg_fd[i],col = color[[target[i]]], lty=lty[[target[i]]], lwd=lwd[[target[i]]])
}
abline(v=reg$land[2],lty=2,lwd=1)
abline(v=reg$land[3],lty=2,lwd=1)
abline(v=reg$land[4],lty=2,lwd=1)
axis(3,tick=F,at=at_land, labels=landlab,cex.axis=1.5)
legend('topleft',legend=unlist(name),col=unlist(color),lwd=unlist(lwd),lty=unlist(lty),cex=1.5)
dev.off()
h_inv_list = list()
for (i in 1:n_levels) {
steps <- seq(norm_rng[1],norm_rng[2],len=50)
hknots <- as.numeric(eval.fd(steps,reg$warpfd[i]))
# rounding error quick fixes
hknots[1] = 0
hknots[length(steps)] = norm_rng[2]
hnbasis=norder + 50 - 2
hbasis <- create.bspline.basis(norm_rng, hnbasis, norder, hknots)
hfdPar <- fdPar(hbasis, Lfdobj, lambda)
h_inv_list[[i]] <- smooth.basis(hknots,steps,hfdPar)$fd
}
png(paste(plots_dir,'registration_lin.png',sep=''))
plot(range(reg$land),c(-3,3),type='n',xlab='element_norm_time',ylab='Residualized F0',las=1,ylim=c(ybot,yup),cex.lab=1.3,cex.axis=1.3)
for (i in subsamp_small) {
lines(res_fd[i],lty=1,col=1)
points(land[i,2],eval.fd(land[i,2],res_fd[i]),col='red', pch=19,cex=1.3)
points(land[i,3],eval.fd(land[i,3],res_fd[i]),col='red', pch=19,cex=1.3)
points(land[i,4],eval.fd(land[i,4],res_fd[i]),col='red', pch=19,cex=1.3)
}
dev.off()
# plot the same curves after registration
png(paste(plots_dir,'registration_land.png',sep=''))
plot(range(reg$land),c(-3,3),type='n',xlab='element_norm_time',ylab='Residualized F0',las=1,ylim=c(ybot,yup),cex.lab=1.3,cex.axis=1.3)
for (i in subsamp_small) {
lines(resreg_fd[i],lty=1,col=1)
t_reg = eval.fd(land[i,2],h_inv_list[[i]])
points(t_reg,eval.fd(t_reg,resreg_fd[i]),col='red', pch=19,cex=1.3)
t_reg = eval.fd(land[i,3],h_inv_list[[i]])
points(t_reg,eval.fd(t_reg,resreg_fd[i]),col='red', pch=19,cex=1.3)
t_reg = eval.fd(land[i,4],h_inv_list[[i]])
points(t_reg,eval.fd(t_reg,resreg_fd[i]),col='red', pch=19,cex=1.3)
}
abline(v=reg$land[2],lty=2,col='black',lwd=1)
abline(v=reg$land[3],lty=2,col='black',lwd=1)
abline(v=reg$land[4],lty=2,col='black',lwd=1)
axis(3,tick=F,at=at_land, labels=landlab,cex.axis=1.5)
dev.off()
y_fd = resreg_fd # alias, line 420
lambda_pca    <- lambda
pcafdPar  <- fdPar(basis_res,int2Lfd(2), lambda_pca) #basis_f0 at line 320
res_pcafd <- pca.fd(y_fd, nharm=3, pcafdPar) # first three PCs
#check PC1
plot(res_pcafd$harmonics[1],xlab='Element_norm_time ',ylab='Residualized F0',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='black',lwd=3,)
f0_s1 = res_pcafd$scores[,1]
f0_s2 = res_pcafd$scores[,2]
stressed_time_matrix=matrix(unlist(stressed_time_list), byrow=TRUE, nrow=length(stressed_time_list) )
item1=data.frame(discourse_list,speakers,stressed_time_matrix,target,f0_s1,f0_s2)
classes=item1$target
plot.pca.fd.corr(res_pcafd,xlab = 'element_norm_time',ylab='Residualized F0',land = reg$land , nx=40,plots_dir = plots_dir, basename = 'PCA_f0reg_',height=480)
png(paste(plots_dir,'PCscatter_f0reg.png',sep=''))
xyplot(res_pcafd$scores[,2] ~  res_pcafd$scores[,1] , cex=1.5,
xlab = list(label=expression(s[1]),cex=2),ylab= list(label=expression(s[2]),cex=2),
groups= classes,
pch  = sapply(levels(factor(classes)), function(x) symbol[[x]],USE.NAMES = FALSE),
col  = sapply(levels(factor(classes)), function(x) color[[x]],USE.NAMES = FALSE),
,scales = list(cex=1.5)
)
dev.off()
data_dir =  paste(root_dir,'data/',sep='')
res_str_data = read.csv(file = paste(data_dir,"res_structure",sep=''))
res_str_data = read.csv(file = paste(data_dir,"res_structure.csv",sep=''))
speakers = levels(res_str_data$speakers)
f0_s1_class.lm = lm(f0_s1 ~ target,  data = res_str_data)
summary(f0_s1_class.lm) #R^2 =  .015
f0_s1_spk.lm = lm(f0_s1 ~ speakers,  data = res_str_data)
summary(f0_s1_spk.lm) #R^2 = .74
f0_s1_class.p =  data.frame(spk=speakers,p=rep(NA,length(speakers)),row.names=speakers)
for (spk in f0_s1_class.p$spk) {
f0_s1_class.p[spk,'p'] = t.test( f0_s1 ~ target,  data = res_str_data[which(res_str_data$speakers==spk),],alternative='greater')$p.value
}
res_int_data = read.csv(file = paste(data_dir,"res_Intonation.csv",sep=''))
f0_s1_class.lm = lm(f0_s1 ~ target,  data = res_int_data)
summary(f0_s1_class.lm) #R^2 =  0.001074,p=0.5405
f0_s1_spk.lm = lm(f0_s1 ~ speakers,  data = res_int_data)
summary(f0_s1_spk.lm) #R^2 =  0.0002312,p=0.7765
png(paste(plots_dir,'PC1','.png',sep=''))
plot(res_pcafd$harmonics[1],xlab='Element_norm_time ',ylab='Residualized F0',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='black',lwd=3,)
abline(v=reg$land[2],lty=2,col='black',lwd=1)
abline(v=reg$land[3],lty=2,col='black',lwd=1)
abline(v=reg$land[4],lty=2,col='black',lwd=1)
axis(3,tick=F,at=at_land, labels=landlab,cex.axis=1.5)
dev.off()
png(paste(plots_dir,'PC2','.png',sep=''))
plot(res_pcafd$harmonics[2],xlab='Element_norm_time ',ylab='Residualized F0',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='black',lwd=3,)
abline(v=reg$land[2],lty=2,col='black',lwd=1)
abline(v=reg$land[3],lty=2,col='black',lwd=1)
abline(v=reg$land[4],lty=2,col='black',lwd=1)
axis(3,tick=F,at=at_land, labels=landlab,cex.axis=1.5)
dev.off()
png(paste(plots_dir,'PC3','.png',sep=''))
plot(res_pcafd$harmonics[3],xlab='Element_norm_time ',ylab='Residualized F0',main = '',las=1,cex.axis=1.5,cex.lab=1.5,col='black',lwd=3,)
abline(v=reg$land[2],lty=2,col='black',lwd=1)
abline(v=reg$land[3],lty=2,col='black',lwd=1)
abline(v=reg$land[4],lty=2,col='black',lwd=1)
axis(3,tick=F,at=at_land, labels=landlab,cex.axis=1.5)
dev.off()
f0_s1_class.lm = lm(f0_s1 ~ target,  data = res_str_data)
summary(f0_s1_class.lm) #R^2 =  0.001074,p=0.5405
focus_data = read.csv(file = paste(data_dir,"focus_item1.csv",sep=''))
f0_s1_class.lm = lm(f0_s1 ~ focus,  data = focus_data)
summary(f0_s1_class.lm) #R^2 =  0.001074,p=0.5405
